{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problem_name = 'roads' #to save\n",
    "model_architecture = 'unet'\n",
    "weights_path = None \n",
    "target_size = (256, 256) \n",
    "batch_size = 2\n",
    "\n",
    "epochs = 100 #após x épocas sem melhorar pará (a usar callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Conv2D, Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.constraints import maxnorm \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "from keras import backend as K \n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "import keras\n",
    "K.set_image_dim_ordering('tf') #ordem 'th' ou 'tf' \n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import math \n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from time import time as tick\n",
    "import matplotlib.pyplot as plt \n",
    "import pickle \n",
    "from os import listdir\n",
    "from PIL import Image, ImageOps\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "from scipy.misc\timport toimage \n",
    "from scipy import misc, ndimage\n",
    "import scipy.fftpack as pack\n",
    "import scipy.misc\n",
    "from scipy.ndimage import rotate\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "from keras.layers import (Activation, Conv2D, Conv2DTranspose, Dense, Dropout,\n",
    "                          Flatten, Input, MaxPooling2D, concatenate,\n",
    "                          GlobalAveragePooling2D)\n",
    "\n",
    "# fixar random seed para se puder reproduzir os resultados \n",
    "seed = 9 \n",
    "np.random.seed(seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Útils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_model(model,fich):\n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, to_file=fich, show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "def print_history_accuracy(history):\n",
    "    print(history.history.keys())\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def print_history_loss(history):\n",
    "    print(history.history.keys())\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0,2,3,1))\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, cols, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=16)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')\n",
    "\n",
    "def load_batch(fpath, label_key='labels'): \n",
    " \n",
    "    f = open(fpath, 'rb') \n",
    "    d = pickle.load(f, encoding='bytes') \n",
    "    d_decoded = {}        # decode utf8 \n",
    "    for k, v in d.items(): \n",
    "        d_decoded[k.decode('utf8')] = v \n",
    "    d = d_decoded \n",
    "    f.close() \n",
    "    data = d['data'] \n",
    "    labels = d[label_key] \n",
    "    data = data.reshape(data.shape[0], 3, 32, 32) \n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def rotate_resize(temp, tam_image):\n",
    "    #-------------------rodar se necessário e cortar em quadrado\n",
    "    if temp.shape[0] > temp.shape[1]:\n",
    "        temp = rotate(temp,90)\n",
    "    \n",
    "    #cortar em quadrado no centro da imagem e fazer resize para o tam_image\n",
    "    difShapes = temp.shape[1]-temp.shape[0]\n",
    "    return (255 * resize(temp[0:temp.shape[0],int(difShapes/2):int(difShapes/2)+temp.shape[0]],\n",
    "                            (tam_image, tam_image))).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#choosing model\n",
    "\n",
    "def choosing_model(model_architecture, num_classes,epochs, weights_path=None):\n",
    "    \n",
    "    model = None\n",
    "    \n",
    "    if model_architecture == 'unet':\n",
    "        model = unet(input_tensor, input_shape=None, nb_classes=None)\n",
    "    if model == None:\n",
    "        print('non valid model')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unet(input_tensor, input_shape=None, nb_classes=None):\n",
    "    # Makes 4 downsamplings\n",
    "    # 2^4=16 » Minimum image size allowed is 32x32\n",
    "    print(\"Architecture: U-Net\")\n",
    "    conv1 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input_tensor)\n",
    "    conv1 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(conv5)\n",
    "\n",
    "    up6 = concatenate(\n",
    "        [\n",
    "            Conv2DTranspose(256, (2, 2), strides=(2, 2),\n",
    "                            padding=\"same\")(conv5), conv4\n",
    "        ],\n",
    "        axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(conv6)\n",
    "\n",
    "    up7 = concatenate(\n",
    "        [\n",
    "            Conv2DTranspose(128, (2, 2), strides=(2, 2),\n",
    "                            padding=\"same\")(conv6), conv3\n",
    "        ],\n",
    "        axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(conv7)\n",
    "\n",
    "    up8 = concatenate(\n",
    "        [\n",
    "            Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=\"same\")(conv7),\n",
    "            conv2\n",
    "        ],\n",
    "        axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(conv8)\n",
    "\n",
    "    up9 = concatenate(\n",
    "        [\n",
    "            Conv2DTranspose(32, (2, 2), strides=(2, 2), padding=\"same\")(conv8),\n",
    "            conv1\n",
    "        ],\n",
    "        axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(conv9)\n",
    "\n",
    "    conv10 = Conv2D(3, (1, 1), activation=\"sigmoid\")(conv9)\n",
    "\n",
    "    model = Model(inputs=[input_tensor], outputs=[conv10])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=1e-4),\n",
    "        loss=[\"mean_squared_error\"],\n",
    "        metrics=[\"mean_squared_error\"])\n",
    "\n",
    "    print(model.summary())\n",
    "    print(\"Architecture: U-Net\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "history_loss = LossHistory() #print(history.losses) to use      \n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, mode='min')    \n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath = 'checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=False, mode='min', period=1)\n",
    "\n",
    "#reduce training rate when no improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "csv_logger = CSVLogger('training.log')\n",
    "\n",
    "\n",
    "\n",
    "# Catch SIGINT (same as 'Ctrl+C') signals handler\n",
    "class SignalStopping(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Stop training when an interrupt signal (or other) was received\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sig=signal.SIGINT, doubleSignalExits=False, verbose=0):\n",
    "        \"\"\"\n",
    "        Stop training when an interrupt signal (or other) was received\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            sig {signal} -- The signal to listen to. Defaults to signal.SIGINT. (default: {signal.SIGINT})\n",
    "            doubleSignalExits {bool} -- Receiving the signal twice exits the python process instead of waiting for this epoch to finish. (default: {False})\n",
    "            verbose {int} -- Verbosity mode. (default: {0})\n",
    "        \"\"\"\n",
    "\n",
    "        super(SignalStopping, self).__init__()\n",
    "\n",
    "        self.signal_received = False\n",
    "        self.verbose = verbose\n",
    "        self.doubleSignalExits = doubleSignalExits\n",
    "\n",
    "        def signal_handler(sig, frame):\n",
    "            if self.signal_received and self.doubleSignalExits:\n",
    "                if self.verbose > 0:\n",
    "                    print(\n",
    "                        \"\"\n",
    "                    )  #new line to not print on current status bar. Better solution?\n",
    "                    print(\"Received signal to stop \" + str(sig) +\n",
    "                          \" twice. Exiting..\")\n",
    "                exit(sig)\n",
    "\n",
    "            self.signal_received = True\n",
    "            if self.verbose > 0:\n",
    "                print(\n",
    "                    \"\"\n",
    "                )  #new line to not print on current status bar. Better solution?\n",
    "                print(\"Received signal to stop: \" + str(sig))\n",
    "\n",
    "        signal.signal(signal.SIGINT, signal_handler)\n",
    "        self.stopped_epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if self.signal_received:\n",
    "            self.stopped_epoch = epoch\n",
    "            self.model.stop_training = True\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        if self.stopped_epoch > 0 and self.verbose > 0:\n",
    "            print(\"Epoch %05d: stopping due to signal\" % (self.stopped_epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(num_train, num_validation, model_architecture, train_generator, validation_generator, epochs, batch_size, weights_path=None):\n",
    "    \n",
    "    #num_classes = train_generator.num_classes\n",
    "    \n",
    "    input_tensor = Input(shape=(256, 256, 3)) \n",
    "    input_shape=(256, 256, 3)\n",
    "    nb_classes=1\n",
    "    \n",
    "    model = unet(input_tensor, input_shape, nb_classes) \n",
    "    \n",
    "    print(model.summary())\n",
    "    #print_model(model,\"model_plus.png\")\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch = math.ceil(num_train/batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps = math.ceil(num_validation/batch_size),\n",
    "        callbacks=[SignalStopping, history_loss, early_stop, checkpoint, reduce_lr, csv_logger],\n",
    "        verbose=1) \n",
    "    \n",
    "    model.save(model, problem_name + '_' + model_architecture + '.h5') \n",
    "    \n",
    "    print_history_accuracy(history) \n",
    "    print_history_loss(history) \n",
    "    # Final evaluation with test cases\n",
    "    scores = model.evaluate_generator(validation_generator) \n",
    "    print('Scores: ', scores) \n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100)) \n",
    "    print(\"Erro modelo: %.2f%%\" % (100-scores[1]*100))\n",
    "    \n",
    "    return model, history \n",
    "\n",
    "def training_folders(model_architecture, trainPath, testPath, target_size, epochs, batch_size, weights_path=None):\n",
    "    \n",
    "    # we create two instances with the same arguments\n",
    "    data_gen_args = dict(validation_split = 0.1,#featurewise_center=True,\n",
    "                         #featurewise_std_normalization=True,\n",
    "                         rotation_range=20.,\n",
    "                         width_shift_range=0.1,\n",
    "                         height_shift_range=0.1,\n",
    "                         zoom_range=[1.5,1.5])\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
    "    seed = 1\n",
    "    #image_datagen.fit(images, augment=True, seed=seed)\n",
    "    #mask_datagen.fit(masks, augment=True, seed=seed)\n",
    "\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        trainPath + 'images',\n",
    "        class_mode=None,\n",
    "        seed=seed,\n",
    "        target_size=target_size,\n",
    "        color_mode = 'rgb',\n",
    "        batch_size=batch_size,\n",
    "        #shuffle = True)\n",
    "        subset = 'training'\n",
    "    )\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        'train/masks',\n",
    "        class_mode=None,\n",
    "        seed=seed,\n",
    "        target_size=target_size,\n",
    "        color_mode = 'rgb',\n",
    "        batch_size=batch_size,\n",
    "        #shuffle = True)\n",
    "        subset = 'training'\n",
    "    )\n",
    "    \n",
    "    # combine generators into one which yields image and masks\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    \n",
    "    test_image_generator = image_datagen.flow_from_directory(\n",
    "        'train/images',\n",
    "        class_mode=None,\n",
    "        seed=seed,\n",
    "        target_size=target_size,\n",
    "        color_mode = 'rgb',\n",
    "        batch_size=batch_size,\n",
    "        #shuffle = True)\n",
    "        subset = 'validation'\n",
    "    )\n",
    "    \n",
    "    test_mask_generator = mask_datagen.flow_from_directory(\n",
    "        'train/masks',\n",
    "        class_mode=None,\n",
    "        seed=seed,\n",
    "        target_size=target_size,\n",
    "        color_mode = 'rgb',\n",
    "        batch_size=batch_size,\n",
    "        #shuffle = True)\n",
    "        subset = 'validation'\n",
    "    )\n",
    "    \n",
    "    # combine generators into one which yields image and masks\n",
    "    test_generator = zip(test_image_generator, test_mask_generator)\n",
    "\n",
    "    num_train = image_generator.n\n",
    "    num_validation = test_image_generator.n\n",
    "    '''  \n",
    "    train_datagen = ImageDataGenerator(         #https://keras.io/preprocessing/image/\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255)\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "            trainPath,\n",
    "            target_size=target_size,\n",
    "            color_mode = 'rgb',\n",
    "            batch_size=batch_size,\n",
    "            class_mode = None,\n",
    "            shuffle = True)\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "            testPath,\n",
    "            target_size = target_size,\n",
    "            color_mode = 'rgb',\n",
    "            class_mode = None,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = True)\n",
    "    ''' \n",
    "    model, history = training(num_train, num_validation, model_architecture, train_generator, test_generator, epochs, batch_size, weights_path)\n",
    "    \n",
    "    return model, history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2340 images belonging to 1 classes.\n",
      "Found 2340 images belonging to 1 classes.\n",
      "Found 260 images belonging to 1 classes.\n",
      "Found 260 images belonging to 1 classes.\n",
      "Architecture: U-Net\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 256, 256, 32) 896         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DTran (None, 32, 32, 256)  524544      conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 32, 32, 512)  0           conv2d_transpose_21[0][0]        \n",
      "                                                                 conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 32, 32, 256)  1179904     concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_22 (Conv2DTran (None, 64, 64, 128)  131200      conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 64, 64, 256)  0           conv2d_transpose_22[0][0]        \n",
      "                                                                 conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 64, 64, 128)  295040      concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_23 (Conv2DTran (None, 128, 128, 64) 32832       conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 128, 128, 128 0           conv2d_transpose_23[0][0]        \n",
      "                                                                 conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 128, 128, 64) 73792       concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_24 (Conv2DTran (None, 256, 256, 32) 8224        conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 256, 256, 64) 0           conv2d_transpose_24[0][0]        \n",
      "                                                                 conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 256, 256, 32) 18464       concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 256, 256, 3)  99          conv2d_133[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 7,760,163\n",
      "Trainable params: 7,760,163\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Architecture: U-Net\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 256, 256, 32) 896         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DTran (None, 32, 32, 256)  524544      conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 32, 32, 512)  0           conv2d_transpose_21[0][0]        \n",
      "                                                                 conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 32, 32, 256)  1179904     concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_22 (Conv2DTran (None, 64, 64, 128)  131200      conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 64, 64, 256)  0           conv2d_transpose_22[0][0]        \n",
      "                                                                 conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 64, 64, 128)  295040      concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_129 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_23 (Conv2DTran (None, 128, 128, 64) 32832       conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 128, 128, 128 0           conv2d_transpose_23[0][0]        \n",
      "                                                                 conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 128, 128, 64) 73792       concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_24 (Conv2DTran (None, 256, 256, 32) 8224        conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 256, 256, 64) 0           conv2d_transpose_24[0][0]        \n",
      "                                                                 conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 256, 256, 32) 18464       concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 256, 256, 3)  99          conv2d_133[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 7,760,163\n",
      "Trainable params: 7,760,163\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "                             \n",
    "    model, history = training_folders(model_architecture, trainPath, testPath, target_size, epochs, batch_size, weights_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageDataGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-05a6ab4ab0fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_datagen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'S1/test/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m validation_generator = test_datagen.flow_from_directory(\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = 'VGG_16'\n",
    "testPath = 'test/'\n",
    "\n",
    "model = load_model('Models/' + model_name + 'model.h5')\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        testPath,\n",
    "        color_mode = 'rgb',\n",
    "        class_mode = 'categorical',\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)  # keep data in same order as labels\n",
    "\n",
    "\n",
    "# Final evaluation with test cases\n",
    "scores = model.evaluate_generator(validation_generator) \n",
    "print('Scores: ', scores) \n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100)) \n",
    "print(\"Erro modelo: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "scores = model.predict_generator(validation_generator)\n",
    "j, predicted_classes = np.unravel_index(scores.argmax(axis=1), scores.shape)\n",
    "\n",
    "for i in range(0, scores.shape[0]):\n",
    "    if predicted_classes[i] != validation_generator.classes[i]:#print failed images\n",
    "        plt.imshow(imread(pathtest + validation_generator.filenames[i]))\n",
    "        plt.show()\n",
    "        print('true: ', validation_generator.classes[i])\n",
    "        print('prediction: ',predicted_classes[i])\n",
    "        print('scores: ', scores[i])\n",
    "        print('file: ', validation_generator.filenames[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix + AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(validation_generator.classes, predicted_classes))\n",
    "\n",
    "# AUC for prediction on validation sample\n",
    "X_val_sample, val_labels = next(validation_generator)\n",
    "val_pred = model.predict_proba(X_val_sample)\n",
    "val_pred = np.reshape(val_pred, val_labels.shape)\n",
    "val_score_auc = roc_auc_score(val_labels, val_pred)\n",
    "print (\"AUC validation score: \",val_score_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_to_save = 'testes'\n",
    "n_imgs = 10\n",
    "#select image \n",
    "origin = '/media/marcelo/OS/Users/Marcelo Queirós/Documents/MIEI/Semestre 2/Tecnologias e Aplicacoes/deep learning/Projects/pills/dc/' \n",
    "image_path = origin + '1.jpg'\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=2,\n",
    "    height_shift_range=2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "image = np.expand_dims(ndimage.imread(image_path),0) \n",
    "\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "#model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
    "                    #steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
    "i = 0\n",
    "for batch in datagen.flow(image, batch_size=1, \n",
    "                          save_to_dir = dir_to_save, save_prefix='dc', save_format='jpg'):\n",
    "    i += 1\n",
    "    if i > n_imgs:\n",
    "        break  # otherwise the generator would loop indefinitely\n",
    "        \n",
    "#plots(aug_images, figsize=(20,7), rows=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
